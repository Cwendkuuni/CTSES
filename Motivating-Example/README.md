# Similarity Evaluation for LLM-Refactored Test Cases

This script reproduces the similarity evaluation presented in **Section II: Motivating Example** of our paper. We compare a test suite automatically generated by **EvoSuite** to its refactored version produced by **GPT-4o**, and compute several metrics to highlight the limitations of lexical-based similarity scores.

## Context

The motivating example focuses on a real test case from the **SF110 benchmark**, namely the `MacawWorkBench_ESTest` class from project `69_lhamacaw`. The original version was generated by EvoSuite, and the refactored version was obtained using GPT-4o with a Chain-of-Thought prompt designed to improve readability and naming while preserving behavior.

Although the functional behavior is identical, the versions differ in naming, structure, and clarity.

## Files

- `evaluate_similarity_metrics.py`: main script to compute similarity metrics.
- `requirements.txt`: list of required Python packages.
- All data is **hardcoded in the script**, including both the original and the refactored test as string literals.

## Metrics Computed

The script computes three widely used similarity metrics:

| Metric       | Description                                                                 |
|--------------|-----------------------------------------------------------------------------|
| **CodeBLEU** | Combines lexical n-gram, syntax tree, and dataflow similarity               |
| **ROUGE-L**  | Measures longest common subsequence (F1 score) between raw text sequences   |
| **METEOR**   | Accounts for exact, stemmed, and synonym matches at the word level          |

## How to Run

Install dependencies in a virtual environment:

```bash
python -m venv ctses_env
source ctses_env/bin/activate
pip install -r requirements.txt
```

Then run the evaluation script:

```bash
python evaluate_similarity_metrics.py
```

You should obtain the following scores (as reported in the paper):

```
================================================================================
Evaluating similarity between EvoSuite and GPT-refactored test classes
================================================================================
[CodeBLEU]  Score             : 0.4334213722227591
  - N-Gram Match             : 0.2539057801979098
  - Weighted N-Gram         : 0.4333935166758667
  - Syntax Match            : 0.6019417475728155
  - Dataflow Match          : 0.4444444444444444
[ROUGE-L]  F1 Measure        : 0.5676274944567629
[METEOR]   Score             : 0.6546147527401318
================================================================================
```

## Interpretation

Despite being functionally equivalent, the two test suites differ significantly in naming, structure, and readability:

- **CodeBLEU** underperforms due to heavy penalization of lexical deviations (e.g., renaming `test0` to `testMainMethodThrows...`).
- **ROUGE-L** and **METEOR** reflect partial alignment at the lexical level.
- In the paper, we also report cosine similarities using embeddings (CodeBERT, GraphCodeBERT, OpenAI) — all above **0.96** — which better capture semantic equivalence.

## Reference

This example is described in:

- "Beyond Surface Similarity: Evaluating LLM-Based Test Refactorings"
- Section II: Motivating Example
- Figure: Listing 1 & 2
- Table: Similarity scores (Table 1)

## Notes

The script uses hardcoded test data to ensure reproducibility.

It does not rely on reading .java files to avoid discrepancies due to encoding or formatting.

The version of CodeBLEU used is a direct port of the official repository: https://github.com/microsoft/CodeXGLUE
