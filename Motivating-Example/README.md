# Similarity Evaluation for LLM-Refactored Test Cases

This script reproduces the similarity evaluation presented in **Section II: Motivating Example** of our paper. We compare a test suite automatically generated by **EvoSuite** to its refactored version produced by **GPT-4o**, and compute several metrics to highlight the limitations of lexical-based similarity scores.

## Context

The motivating example focuses on a real test case from the **SF110 benchmark**, namely the `MacawWorkBench_ESTest` class from project `69_lhamacaw`. The original version was generated by EvoSuite, and the refactored version was obtained using GPT-4o with a Chain-of-Thought prompt designed to improve readability and naming while preserving behavior.

Although the functional behavior is identical, the versions differ in naming, structure, and clarity.

## Files

- `evaluate_similarity_metrics.py`: compute lexical similarity metrics (CodeBLEU, ROUGE-L, METEOR).
- `cosine_similarity_metrics.py`: compute semantic similarity using CodeBERT, GraphCodeBERT, and OpenAI embeddings.
- `requirements.txt`: list of required Python packages.
- All data is **hardcoded in the scripts**, including both the original and the refactored test as string literals.

## Metrics Computed

The scripts compute the following similarity metrics:

| Metric           | Type       | Description                                                                 |
|------------------|------------|-----------------------------------------------------------------------------|
| **CodeBLEU**     | Lexical    | Combines lexical n-gram, syntax tree, and dataflow similarity               |
| **ROUGE-L**      | Lexical    | Measures longest common subsequence (F1 score) between raw text sequences   |
| **METEOR**       | Lexical    | Accounts for exact, stemmed, and synonym matches at the word level          |
| **CodeBERT**     | Semantic   | Cosine similarity of transformer-based embeddings (microsoft/codebert-base) |
| **GraphCodeBERT**| Semantic   | Cosine similarity of transformer-based embeddings (microsoft/graphcodebert) |
| **OpenAI**       | Semantic   | Cosine similarity using `text-embedding-3-small` model via OpenAI API       |

## How to Run

Install dependencies in a virtual environment:

```bash
python -m venv ctses_env
source ctses_env/bin/activate
pip install -r requirements.txt
```

Then run the evaluation scripts:

```bash
python evaluate_similarity_metrics.py
python cosine_similarity_metrics.py
```

You should obtain the following scores (as reported in the updated evaluation):

```
================================================================================
Evaluating similarity between EvoSuite and GPT-refactored test classes
================================================================================
[CodeBLEU]  Score             : 0.4334213722227591
  - N-Gram Match             : 0.2539057801979098
  - Weighted N-Gram         : 0.4333935166758667
  - Syntax Match            : 0.6019417475728155
  - Dataflow Match          : 0.4444444444444444
[ROUGE-L]  F1 Measure        : 0.5676274944567629
[METEOR]   Score             : 0.6546147527401318

================================================================================
Evaluating cosine similarity between EvoSuite and GPT-4o refactored test
================================================================================
[CodeBERT]        Cosine Similarity : 0.9989
[GraphCodeBERT]   Cosine Similarity : 0.9967
[OpenAI]          Cosine Similarity : 0.9701
================================================================================
```

## Interpretation

Despite being functionally equivalent, the two test suites differ significantly in naming, structure, and readability:

- **CodeBLEU** underperforms due to heavy penalization of lexical deviations (e.g., renaming `test0` to `testMainMethodThrows...`).
- **ROUGE-L** and **METEOR** reflect partial alignment at the lexical level.
- **Cosine similarities using embeddings** (CodeBERT, GraphCodeBERT, OpenAI) are all above **0.96**, confirming strong semantic equivalence.

## Reference

This example is described in:

- "Beyond Surface Similarity: Evaluating LLM-Based Test Refactorings"
- Section II: Motivating Example
- Figure: Listing 1 & 2
- Table: Similarity scores (Table 1)

## Notes

- The scripts use hardcoded test data to ensure reproducibility.
- No `.java` file reading is needed to avoid encoding or formatting issues.
- CodeBLEU version is from the official Microsoft repo: https://github.com/microsoft/CodeXGLUE
- OpenAI embeddings require a `.env` file with a valid `OPENAI_API_KEY`.
