# CTSES: Composite Test Similarity Evaluation Score

This repository contains the full replication package for our paper:

> **Evaluating LLM-Based Test Refactoring with a Behavior-Preserving Composite Metric**  

CTSES is a **composite similarity metric** tailored for evaluating unit test **refactorings** produced by Large Language Models (LLMs). It integrates **CodeBLEU**, **METEOR**, and **ROUGE-L** to better assess **readability**, **structural clarity**, and **semantic preservation** in test code‚Äîaddressing the limitations of surface-level metrics like CodeBLEU.

---

## Why CTSES?

Traditional metrics fail to capture key refactoring goals:

- **CodeBLEU** penalizes renaming and structural edits  
- **Cosine similarity** captures semantics but ignores readability  
- **ROUGE-L / METEOR** capture lexical overlap but miss behavior  

**CTSES** combines their strengths into a unified score that aligns with developer intuition.

---

## üìÅ Repository Structure

```bash
CTSES/
‚îú‚îÄ‚îÄ Approach/                          # Implementation of CTSES and baselines
‚îú‚îÄ‚îÄ Dataset/                           # JSON files: original EvoSuite test suites
‚îú‚îÄ‚îÄ EvoSuite-Generated-Test-Suite/     # Raw generated tests + stats
‚îú‚îÄ‚îÄ LLM-Refactored-Test-Suite/         # Refactored tests (GPT-4o, Mistral-Large-2407)
‚îú‚îÄ‚îÄ Developer_Aligned_Validation/      # Human validation protocol, annotations, results
‚îú‚îÄ‚îÄ Motivating-Example/                # Section II example reproduction
‚îú‚îÄ‚îÄ Results/                           # Evaluation outputs and aggregated metrics
‚îî‚îÄ‚îÄ README.md                          # This file
```

Each subfolder has its own `README.md` with local documentation and execution steps.

---

## Experimental Setup

We refactored over 5,000 unit tests from two standard benchmarks:

| Dataset   | Projects | Test Classes | Generated by | Refactored by                   |
|-----------|----------|--------------|-------------|--------------------------------|
| Defects4J | 15       | 147          | EvoSuite    | GPT-4o, Mistral-Large-2407     |
| SF110     | 69       | 203          | EvoSuite    | GPT-4o, Mistral-Large-2407     |

All refactorings use Chain-of-Thought prompting to preserve behavior and improve structure.

---

## Metric Design: CTSES

CTSES combines three metrics:

| Metric    | Type       | Captures                          |
|-----------|------------|-----------------------------------|
| CodeBLEU  | Semantic   | Syntactic + dataflow intent       |
| METEOR    | Lexical    | Synonyms, stems, order            |
| ROUGE-L   | Structural | Longest common subsequence        |

**Configurations:**
- **AVG** = (CodeBLEU + METEOR + ROUGE-L) / 3  
- **CTSES1 (Semantic-Prioritized)** = 0.5√óCodeBLEU + 0.3√óMETEOR + 0.2√óROUGE-L  
- **CTSES2 (Readability-Aware)** = 0.4√óCodeBLEU + 0.3√óMETEOR + 0.3√óROUGE-L  

---

## Developer-Aligned Validation

To validate CTSES against human expectations, we conducted a **developer-centered study**:

- **15 representative refactorings** (9 from Defects4J, 6 from SF110)  
- Refactored by **GPT-4o (8 cases)** and **Mistral-Large (7 cases)**  
- Annotated independently by **three developers** (senior engineer, PhD student, junior assistant)  
- Labels on **readability** and **behavior preservation**, with majority-vote consensus  

We compared human labels with CodeBLEU, AVG, and CTSES variants using **MAE** and **false negatives**.  
All artifacts (annotations, spreadsheets, analysis scripts) are provided in `Developer_Aligned_Validation/`.

---

## How to Run the Evaluation

### 1. Install dependencies
```bash
cd Approach/
pip install -r requirements.txt
```

### 2. Run CTSES on refactored outputs
```bash
python3 evaluate_test_similarity.py
```

### 3. Reproduce Motivating Example (Section II)
```bash
cd ../Motivating-Example/
python evaluate_similarity_metrics.py
python cosine_similarity_metrics.py
```

### 4. Run Developer-Aligned Validation
```bash
cd ../Developer_Aligned_Validation/
python analyse_human_evaluation.py
```

### 5. Run Refactoring Pipeline (LLMs)
```bash
cd ../LLM-Refactored-Test-Suite/Refactored-Test-Suite-Code/
bash gpt-scenario-1.sh
bash mistral-scenario-1.sh
```

---

## Sample CTSES Output

```
[CodeBLEU]  : 0.435
[METEOR]    : 0.654
[ROUGE-L]   : 0.568
[CTSES-1]   : 0.528
[CTSES-2]   : 0.541
```

Despite low CodeBLEU, CTSES highlights readability and semantic preservation, better matching developer judgments.

---

## Key Findings

- CTSES aligns more closely with developer annotations  
- Reduces false negatives compared to CodeBLEU  
- Mistral-Large-2407 outperforms GPT-4o on SF110 in CTSES scores  
- CTSES ‚â• 0.5 strongly correlates with acceptable refactorings  

For full results, see the `Results/` folder and the paper‚Äôs tables.

---

## References

- S. Ren et al., *CodeBLEU: a method for automatic evaluation of code synthesis*, arXiv:2009.10297.  
- S. Banerjee and A. Lavie, *METEOR: An automatic metric for MT evaluation*, ACL Workshop 2005.  
- C.-Y. Lin, *ROUGE: A package for automatic evaluation of summaries*, ACL 2004.  
- Fraser & Arcuri. *EvoSuite: automatic test suite generation*, FSE 2011.  
- Wei et al. *Chain-of-Thought prompting elicits reasoning*, NeurIPS 2022.  
- OpenAI. *GPT-4o System Card*, 2024.  
- Mistral AI. *Mistral-Large-2407*, 2024.  
