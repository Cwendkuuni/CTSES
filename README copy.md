# CTSES: Composite Test Similarity Evaluation Score

This repository contains the full replication package for our paper:

> **Beyond CodeBLEU: Rethinking Test Refactoring Evaluation with Semantics-Driven and Behavior-Preserving Metrics**  

CTSES is a **composite similarity metric** tailored for evaluating unit test **refactorings** produced by Large Language Models (LLMs). It integrates **CodeBLEU**, **METEOR**, and **ROUGE-L** to better assess **readability**, **structural clarity**, and **semantic preservation** in test code‚Äîaddressing the limitations of surface-level metrics like CodeBLEU.

---

## Why CTSES?

Traditional metrics fail to capture key refactoring goals:

- **CodeBLEU** penalizes renaming and structural edits  
- **Cosine similarity** captures semantics but ignores readability  
- **ROUGE-L / METEOR** capture lexical overlap but miss behavior

**CTSES** combines their strengths into a unified score that aligns with developer intuition.

---

## üìÅ Repository Structure

```bash
CTSES/
‚îú‚îÄ‚îÄ Approach/                          # Implementation of CTSES and metrics
‚îú‚îÄ‚îÄ Dataset/                           # JSON files: original EvoSuite test suites
‚îú‚îÄ‚îÄ EvoSuite-Generated-Test-Suite/    # Raw generated tests + stats
‚îú‚îÄ‚îÄ LLM-Refactored-Test-Suite/        # Refactored tests (GPT-4o, Mistral-Large-2407)
‚îú‚îÄ‚îÄ Motivating-Example/               # Section II example reproduction
‚îú‚îÄ‚îÄ Results/                          # Evaluation outputs and metrics
‚îî‚îÄ‚îÄ README.md                         # This file
```

Each subfolder has its own README.md for local documentation and execution steps.

---

## Experimental Setup

We refactored over 5,000 unit tests from two standard benchmarks:

| Dataset   | Projects | Test Classes | Generated by | Refactored by                   |
|-----------|----------|--------------|-------------|--------------------------------|
| Defects4J | 15       | 147          | EvoSuite    | GPT-4o, Mistral-Large-2407    |
| SF110     | 69       | 203          | EvoSuite    | GPT-4o, Mistral-Large-2407    |

All refactorings use Chain-of-Thought prompting to preserve behavior and improve structure.

---

## Metric Design: CTSES

CTSES combines three metrics:

| Metric    | Type       | Captures                          |
|-----------|------------|-----------------------------------|
| CodeBLEU  | Semantic   | Syntactic + dataflow intent       |
| METEOR    | Lexical    | Synonyms, stems, order            |
| ROUGE-L   | Structural | Longest common subsequence        |

**Configurations:**
- **CTSES-AVG** = (CodeBLEU + METEOR + ROUGE-L) / 3
- **CTSES-1** = 0.5√óCodeBLEU + 0.3√óMETEOR + 0.2√óROUGE-L (default)
- **CTSES-2** = 0.4√óCodeBLEU + 0.3√óMETEOR + 0.3√óROUGE-L

---

## How to Run the Evaluation

### 1. Install dependencies
```bash
cd Approach/
pip install -r requirements.txt
```

### 2. Run CTSES on refactored outputs
```bash
python3 evaluate_test_similarity.py
```

### 3. Reproduce Motivating Example (Section II)
```bash
cd ../Motivating-Example/
python evaluate_similarity_metrics.py
python cosine_similarity_metrics.py
```

### 4. Run Refactoring Pipeline (LLMs)
```bash
cd ../LLM-Refactored-Test-Suite/Refactored-Test-Suite-Code/
bash gpt-scenario-1.sh
bash mistral-scenario-1.sh
```

---

## Sample CTSES Output

```
[CodeBLEU]  : 0.435
[METEOR]    : 0.654
[ROUGE-L]   : 0.568
[CTSES-1]   : 0.528
[CodeBERT]  : 0.998
[OpenAI]    : 0.949
```

Despite low CodeBLEU, CTSES captures the refactoring's semantic preservation and readability gain.

---

## Key Findings (from the paper)

- CTSES aligns more closely with developer judgments
- Reduces false negatives from CodeBLEU
- Mistral-Large-2407 outperforms GPT-4o on SF110 in CTSES scores
- CTSES ‚â• 0.5 strongly correlates with acceptable refactorings

For full results, see the Results/ folder and Tables III‚ÄìVIII in the paper.

---

## References

- S. Ren et al., *CodeBLEU: a method for automatic evaluation of code synthesis*, arXiv:2009.10297.
- S. Banerjee and A. Lavie, *METEOR: An automatic metric for MT evaluation*, ACL Workshop 2005.
- C.-Y. Lin, *ROUGE: A package for automatic evaluation of summaries*, ACL 2004.
- Z. Feng et al., *CodeBERT: A pre-trained model for programming and natural languages*, arXiv:2002.08155.
- D. Guo et al., *GraphCodeBERT: Pre-training code representations with data flow*, arXiv:2009.08366.
- Fraser & Arcuri. *EvoSuite: automatic test suite generation*, FSE 2011.
- Wei et al. *Chain-of-Thought prompting elicits reasoning*, NeurIPS 2022.
- OpenAI. *GPT-4o System Card*, 2024.
- Mistral AI. *Mistral-Large-2407*, 2024.