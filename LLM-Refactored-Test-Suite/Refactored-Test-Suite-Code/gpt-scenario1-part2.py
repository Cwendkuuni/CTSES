import os
import json
import time
import re
from pathlib import Path
from openai import OpenAI, APIError, APIConnectionError, RateLimitError, AuthenticationError, PermissionError
from dotenv import load_dotenv
import tiktoken

# === Configuration ===
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY-1"))
encoding = tiktoken.encoding_for_model("gpt-4o")

MODEL_NAME = "gpt-4o"
MAX_TOKENS = 128000
RESERVED_TOKENS = 16000
TEMPERATURE = 0.1
NUM_ITERATIONS = 3

OUTPUT_DIR = Path("Refactoring-output/Scenario-1")
LOG_DIR = Path("logs")
DATASET_DIR = Path(__file__).resolve().parent / "DATASET"
FLAG_DIR = Path("iteration_flags")

# === Utility functions ===
def log_error(filename, message):
    LOG_DIR.mkdir(exist_ok=True)
    filepath = LOG_DIR / f"{filename}.log"
    with open(filepath, "a") as f:
        f.write(f"{message}\n")

def clean_text(text):
    text = ''.join(c for c in text if c.isprintable())
    text = re.sub(r'([^\w\s])\1{2,}', r'\1\1', text)
    text = re.sub(r'\n\s*\n', '\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    return '\n'.join(line.strip() for line in text.strip().splitlines())

def trim_tokens(text):
    max_prompt_tokens = MAX_TOKENS - RESERVED_TOKENS
    tokens = encoding.encode(text)
    return encoding.decode(tokens[:max_prompt_tokens]) if len(tokens) > max_prompt_tokens else text

def save_output(dir_path, filename, content):
    dir_path.mkdir(parents=True, exist_ok=True)
    with open(dir_path / filename, "w") as f:
        f.write(content)

# === Prompt generation ===
def build_prompt(static_part, test_code):
    return f"""
You are an expert software engineer with advanced knowledge of Java testing and refactoring. The following test suite was generated by EvoSuite and must be refactored for readability, maintainability, and modularity, while removing identified test smells without altering functionality.

Constraints:
- Do Not Alter: Retain EvoSuite-specific elements (package/import statements, annotations, and class declaration): {static_part}
- Preserve Functionality: Do not change the test behavior.
- Add Given-When-Then Comments: Clarify each test’s structure.

Steps:
1. Understand Test Intent: Briefly describe the class’s purpose and test targets.
2. Analyze Dependencies and Group related logic.
3. Refactor Test Methods:
   - Rename methods and variables descriptively.
   - Add Given-When-Then comments.
4. Review for correctness and maintainability.

Test Suite:
{test_code}

Return only the final refactored code enclosed in triple backticks ``` ``` for easy extraction..
""".strip()

# === OpenAI API call ===
def call_openai(prompt, retries=30):
    prompt = trim_tokens(prompt)
    for attempt in range(retries):
        try:
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=RESERVED_TOKENS,
                temperature=TEMPERATURE
            )
            return response.choices[0].message.content.strip()

        except (APIError, APIConnectionError, RateLimitError, AuthenticationError, PermissionError) as e:
            log_error("openai_error", f"{type(e).__name__}: {e}")
            time.sleep(5 if not isinstance(e, RateLimitError) else 10)

        except Exception as e:
            log_error("unexpected_error", str(e))
            time.sleep(5)
    return None

# === Dataset processing ===
def process_dataset(dataset_path, dataset_name, has_bug_id):
    with open(dataset_path, 'r') as f:
        data = json.load(f)

    for iteration in range(1, NUM_ITERATIONS + 1):
        for entry in data:
            project = entry["project_name"]
            clazz = entry["class"]
            iteration_id = entry.get("iteration", "")
            bug_id = entry.get("bug-id", "")
            test_code = clean_text(entry["test_code"])
            static_part = clean_text(entry["Static_part_to_keep_from_EvoSuite"])

            if not test_code or not static_part:
                log_error(f"{project}_{clazz}_{iteration_id}", "Empty cleaned input")
                continue

            if has_bug_id:
                out_path = OUTPUT_DIR / "GPT" / dataset_name / project / clazz / str(bug_id) / f"testsuite_{iteration_id}"
                filename = f"{iteration_id}-{project}-{bug_id}-{clazz}-refactoring-output-iter-{iteration}.txt"
            else:
                out_path = OUTPUT_DIR / "GPT" / dataset_name / project / clazz / f"testsuite_{iteration_id}"
                filename = f"{iteration_id}-{project}-{clazz}-refactoring-output-iter-{iteration}.txt"

            if (out_path / filename).exists():
                continue

            prompt = build_prompt(static_part, test_code)
            response = call_openai(prompt)

            if response:
                save_output(out_path, filename, response)
            else:
                log_error(filename, "No output generated or request failed")

        FLAG_DIR.mkdir(exist_ok=True)
        flag_file = FLAG_DIR / f"iteration_{iteration}_{Path(__file__).stem}.flag"
        with open(flag_file, "w") as f:
            f.write("completed")

# === Entry point ===
def main():
    datasets = [
        {"name": "Defects4J", "has_bug_id": True},
        {"name": "SF110", "has_bug_id": False}
    ]
    models = ["GPT"]

    for model in models:
        for ds in datasets:
            path = DATASET_DIR / model / f"{ds['name']}_part2.json"
            if not path.exists():
                continue
            process_dataset(path, ds["name"], ds["has_bug_id"])

if __name__ == "__main__":
    main()
