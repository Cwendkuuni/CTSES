import os
import json
import time
from pathlib import Path
from mistralai import Mistral
from dotenv import load_dotenv

# === Configuration ===
load_dotenv()
client = Mistral(api_key=os.getenv("MISTRAL_API_KEY-1"))

MODEL_NAME = "mistral-large-2407"
TEMPERATURE = 0.1
MAX_RETRIES = 30
WAIT_TIME = 10
NUM_ITERATIONS = 3
MAX_PROMPT_TOKENS = 128000
RESERVED_RESPONSE_TOKENS = 16000

OUTPUT_DIR = Path("Refactoring-output/Scenario-1")
LOG_DIR = Path("logs")
DATASET_DIR = Path(__file__).resolve().parent / "DATASET"
FLAG_DIR = Path("iteration_flags")

# === Utility Functions ===
def log_error(filename, message):
    LOG_DIR.mkdir(exist_ok=True)
    filepath = LOG_DIR / f"{filename}.log"
    with open(filepath, "a") as f:
        f.write(f"{message}\n")

def save_output(dir_path, filename, content):
    dir_path.mkdir(parents=True, exist_ok=True)
    with open(dir_path / filename, "w") as f:
        f.write(content)

def trim_prompt(text, limit=MAX_PROMPT_TOKENS):
    return text[:limit] if len(text) > limit else text

# === Prompt Construction ===
def build_prompt(static_part, test_code):
    return f"""
You are an expert software engineer with advanced knowledge of Java testing and refactoring. The following test suite was generated by EvoSuite and must be refactored for readability, maintainability, and modularity without altering functionality.

Constraints:
- Do Not Alter: Retain EvoSuite-specific elements (package/import statements, annotations, and class declaration): {static_part}
- Preserve Functionality
- Add Given-When-Then Comments

Steps:
1. Understand the intent and context of the test suite.
2. Analyze and group dependencies.
3. Refactor methods: rename clearly, restructure with comments.
4. Verify the result.

Test Suite:
{test_code}

Return only the final refactored code enclosed in triple backticks ``` ``` for easy extraction.
""".strip()

# === Mistral API Call ===
def call_mistral(prompt, retries=MAX_RETRIES):
    for attempt in range(retries):
        try:
            response = client.chat.complete(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=TEMPERATURE,
                max_tokens=RESERVED_RESPONSE_TOKENS
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            if attempt < retries - 1:
                time.sleep(WAIT_TIME)
            else:
                log_error("mistral_failure", f"{type(e).__name__}: {e}")
                return None

# === Dataset Processing ===
def process_dataset(dataset_path, dataset_name, has_bug_id):
    with open(dataset_path, 'r') as f:
        data = json.load(f)

    for iteration in range(1, NUM_ITERATIONS + 1):
        for entry in data:
            project = entry["project_name"]
            clazz = entry["class"]
            iteration_id = entry.get("iteration", "")
            bug_id = entry.get("bug-id", "")
            test_code = entry["test_code"]
            static_part = entry["Static_part_to_keep_from_EvoSuite"]

            if has_bug_id:
                out_path = OUTPUT_DIR / "MISTRAL" / dataset_name / project / clazz / str(bug_id) / f"testsuite_{iteration_id}"
                filename = f"{iteration_id}-{project}-{bug_id}-{clazz}-refactoring-output-iter-{iteration}.txt"
            else:
                out_path = OUTPUT_DIR / "MISTRAL" / dataset_name / project / clazz / f"testsuite_{iteration_id}"
                filename = f"{iteration_id}-{project}-{clazz}-refactoring-output-iter-{iteration}.txt"

            output_file = out_path / filename
            if output_file.exists():
                continue

            prompt = build_prompt(static_part, test_code)
            prompt = trim_prompt(prompt)

            result = call_mistral(prompt)
            if result:
                save_output(out_path, filename, result)
            else:
                log_error(filename, "Empty response or failed request")

        FLAG_DIR.mkdir(exist_ok=True)
        with open(FLAG_DIR / f"iteration_{iteration}_{Path(__file__).stem}.flag", "w") as f:
            f.write("completed")

# === Entry Point ===
def main():
    datasets = [
        {"name": "Defects4J", "has_bug_id": True},
        {"name": "SF110", "has_bug_id": False}
    ]
    models = ["MISTRAL"]

    for model in models:
        for ds in datasets:
            path = DATASET_DIR / model / f"{ds['name']}_part2.json"
            if not path.exists():
                continue
            process_dataset(path, ds["name"], ds["has_bug_id"])

if __name__ == "__main__":
    main()
